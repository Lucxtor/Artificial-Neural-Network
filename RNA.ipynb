{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b44086",
   "metadata": {},
   "source": [
    "# Rede Neural Artificial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508f066",
   "metadata": {},
   "source": [
    "## Imports necessários "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73b8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da08870",
   "metadata": {},
   "source": [
    "## Funções de Ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262d92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de ativação e derivadas\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    sigmoid_a = sigmoid(a)\n",
    "    return sigmoid_a * (1 - sigmoid_a)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return np.where(a > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=-1, keepdims=True)\n",
    "    exps = np.exp(z)\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(z):\n",
    "    # TODO: Explicar pq utiliza return 1. Tentar implementar a derivada do softmax\n",
    "    return 1\n",
    "\n",
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def identity_derivative(z):\n",
    "    return np.ones_like(z)\n",
    "\n",
    "activation_funcs = {\n",
    "    'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "    'relu':    (relu, relu_derivative),\n",
    "    'softmax': (softmax, softmax_derivative),\n",
    "    'identity': (identity, identity_derivative)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571f91d",
   "metadata": {},
   "source": [
    "## Funções de Custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d44b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de custo e derivadas\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred, eps=1e-15):\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred, eps=1e-15):\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred)) / y_pred.shape[0] # dividir por predictions.shape[0] tem o intuito de normalizar o valor da perda pelo número de amostras, tornando a perda independente do tamanho do batch\n",
    "\n",
    "def categorical_cross_entropy_derivative(y_true, y_pred, eps=1e-12):\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return y_pred - y_true\n",
    "\n",
    "cost_funcs = {\n",
    "    'mse':            (mse, mse_derivative),\n",
    "    'binary_cross_entropy':  (binary_cross_entropy, binary_cross_entropy_derivative),\n",
    "    'categorical_cross_entropy': (categorical_cross_entropy, categorical_cross_entropy_derivative)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb6ea2",
   "metadata": {},
   "source": [
    "## Inicialização da Rede e Feedfoward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dac7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_layers(layers, c_inputs):\n",
    "  \"\"\"\n",
    "  Inicializa as camadas da rede neural com pesos aleatórios.\n",
    "\n",
    "  Parâmetros\n",
    "  ----------\n",
    "  layers : list of dict\n",
    "      Lista onde cada dicionário contém:\n",
    "      - 'neurons': número de neurônios na camada\n",
    "      - 'activation_function': nome da função de ativação usada na camada\n",
    "  c_inputs : int\n",
    "      Número de entradas da rede (atributos do dataset).\n",
    "\n",
    "  Retorna\n",
    "  -------\n",
    "  initialized_layers : list\n",
    "      Lista de camadas com pesos e função de ativação configurados.\n",
    "  \"\"\"\n",
    "\n",
    "  initialized_layers = []\n",
    "\n",
    "  for i, layer in enumerate(layers):\n",
    "    # se for a primeira camada, o número de entradas é igual ao número de atributos\n",
    "    if i == 0:\n",
    "      input_size = c_inputs\n",
    "    else:\n",
    "      input_size = layers[i - 1]['neurons']\n",
    "    # o número de saídas é igual ao número de neurônios da camada atual\n",
    "    output_size = layer['neurons']\n",
    "    \n",
    "    # Inicializa os pesos com valores aleatórios pequenos e mais um para o viés\n",
    "    weight_matrix = np.random.uniform(low=-0.33, high=0.33,size=(output_size,input_size + 1))\n",
    "  \n",
    "    initialized_layers.append({\n",
    "        'weights': weight_matrix,\n",
    "        'activation_func': layer['activation_function']\n",
    "    })\n",
    "  return initialized_layers\n",
    "\n",
    "def feed_forward(layers, inputs):\n",
    "  \"\"\"\n",
    "  Executa o algoritmo de propagação direta (feedforward) em uma rede neural.\n",
    "\n",
    "  Parâmetros\n",
    "  ----------\n",
    "  layers : list\n",
    "      Lista de camadas já inicializadas com pesos e funções de ativação.\n",
    "  inputs : np.ndarray\n",
    "      Vetor de entrada com os atributos de uma observação.\n",
    "\n",
    "  Retorna\n",
    "  -------\n",
    "  activation : np.ndarray\n",
    "      Saída final da rede após a última camada (predição).\n",
    "  layers : list\n",
    "      Lista atualizada das camadas com entradas e saídas armazenadas para uso posterior no backpropagation.\n",
    "  \"\"\"\n",
    "    \n",
    "  activation = inputs\n",
    "\n",
    "  # Itera sobre cada camada da rede neural\n",
    "  for layer in layers:\n",
    "\n",
    "    # Concatena para o input + viés da matriz de pesos\n",
    "    activation = np.concatenate(([1], activation))\n",
    "    \n",
    "    # salva input na layer\n",
    "    layer['input'] = activation\n",
    "\n",
    "    # Calcula a saída da camada atual: z = w' * a + b, onde w' é a transposta da matriz de pesos, a é o input da camada e b é o viés que foi concatenado\n",
    "    z = np.dot(activation, layer['weights'].T)\n",
    "    \n",
    "    # Extrai e calcula a função de ativação do dicionário activation_funcs\n",
    "    activation_func, _ = activation_funcs[layer['activation_func']]\n",
    "    \n",
    "    activation = activation_func(z)\n",
    "  \n",
    "    # salva a operacao na layer\n",
    "    layer['output'] = z\n",
    "\n",
    "  return activation, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b005a",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea683b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(layers, cost_derivation, learningRate):\n",
    "    \"\"\"\n",
    "    Executa o algoritmo de backpropagation em uma rede neural artificial.\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    layers : list\n",
    "        Lista de dicionários representando as camadas da rede após o feedfoward. Cada camada deve conter:\n",
    "        - \"activation_func\": nome da função de ativação\n",
    "        - \"output\": saída da camada após a ativação\n",
    "        - \"input\": entrada recebida pela camada\n",
    "        - \"weights\": matriz de pesos da camada\n",
    "    cost_derivation : np.ndarray\n",
    "        Derivada da função de custo em relação à saída da rede.\n",
    "    learningRate : float\n",
    "        Taxa de aprendizado usada para atualização dos pesos.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    newWeights : list\n",
    "        Lista contendo os novos pesos atualizados para cada camada.\n",
    "    \"\"\"\n",
    "    newWeights = []\n",
    "    \n",
    "    error = None\n",
    "    nextLayerWeights = None\n",
    "\n",
    "    # Percorre as camadas da rede em ordem reversa (do output para o input)\n",
    "    for layer in reversed(layers):\n",
    "        # Obtém e calcula a derivada da função de ativação da camada atual\n",
    "        _, activation_derivation = activation_funcs[layer[\"activation_func\"]]\n",
    "\n",
    "        derivation = activation_derivation(layer[\"output\"])\n",
    "\n",
    "        if error is None:\n",
    "            # Primeira iteração: erro é a derivada do custo vezes a derivada da ativação\n",
    "            error = cost_derivation * derivation\n",
    "        else:\n",
    "            # Para camadas intermediárias: propaga o erro da camada seguinte\n",
    "            propagated_error = np.dot(error, nextLayerWeights[:, 1:]) # Ignora o viés\n",
    "\n",
    "            error = propagated_error * derivation\n",
    "\n",
    "        # Calcula e aplica o gradiente\n",
    "        gradient = np.outer(error, layer[\"input\"]) * learningRate\n",
    "\n",
    "        nextLayerWeights = layer[\"weights\"]\n",
    "\n",
    "        newLayerWeights = nextLayerWeights - gradient\n",
    "\n",
    "        newWeights.insert(0, newLayerWeights)\n",
    "    \n",
    "    return newWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4da28",
   "metadata": {},
   "source": [
    "## Função de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e942e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ann, epochs, x, y, learning_rate, cost_func_name):\n",
    "    \"\"\"\n",
    "    Executa o treinamento de uma rede neural artificial (RNA).\n",
    "\n",
    "    Parâmetros:\n",
    "    ann : list\n",
    "        Estrutura da rede neural, lista de dict {weights, activation_func}.\n",
    "    epochs : int\n",
    "        Número de épocas de treinamento.\n",
    "    x : pandas.DataFrame\n",
    "        Conjunto de dados de entrada.\n",
    "    y : list or np.ndarray\n",
    "        Conjunto de saídas desejadas.\n",
    "    learning_rate : float\n",
    "        Taxa de aprendizado.\n",
    "    cost_func_name : string\n",
    "        Nome da função de custo desejada.\n",
    "\n",
    "    Retorna:\n",
    "    stages : list\n",
    "        Lista contendo os estados da RNA ao final de cada época.\n",
    "    \"\"\"\n",
    "\n",
    "    stages = []  # Armazena o estado da rede após cada época\n",
    "\n",
    "    # Loop sobre cada época\n",
    "    for _ in range(epochs):\n",
    "        # Loop sobre cada observação no conjunto de dados\n",
    "        for observation_id in range(len(x)):\n",
    "\n",
    "            observation = np.array(x.iloc[observation_id])\n",
    "\n",
    "            # Executa a propagação direta\n",
    "            y_pred, ann = feed_forward(ann, observation)\n",
    "\n",
    "            y_true = np.array(y[observation_id])\n",
    "            \n",
    "            _, cost_derivation_func = cost_funcs[cost_func_name]\n",
    "\n",
    "            # Calcula da saída\n",
    "            cost_derivation = cost_derivation_func(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "            # Executa a retropropagação para atualizar os pesos\n",
    "            newWeights = backpropagation(\n",
    "                layers=ann,\n",
    "                cost_derivation=cost_derivation,\n",
    "                learningRate=learning_rate\n",
    "            )\n",
    "\n",
    "            for i, layer in enumerate(ann):\n",
    "                layer[\"weights\"] = newWeights[i]\n",
    "\n",
    "        # Salva o estado da rede ao final da época\n",
    "        stages.append(ann)\n",
    "\n",
    "    return stages[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d03f9a",
   "metadata": {},
   "source": [
    "## Funções de Preparação de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16791f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: trocar dataset https://www.kaggle.com/datasets/adityakadiwal/water-potability\n",
    "\n",
    "def prepareDataBinaryClassification():\n",
    "  # print(data.head())\n",
    "  DATA_PATH = 'prepareData/penguins_binary_classification.csv'\n",
    "\n",
    "  data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "  # Realizando o label encoding da coluna 'species'\n",
    "  data['species'] = data['species'].map({'Adelie': 0, 'Gentoo': 1})\n",
    "\n",
    "  # Realizando o label encoding da coluna 'island'\n",
    "  data['island'] = data['island'].map({'Torgersen': 0, 'Biscoe': 1, 'Dream': 2})\n",
    "\n",
    "  numerical_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'year']\n",
    "\n",
    "  # Inicializando o MinMaxScaler\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "  # Aplicando a normalização às colunas numéricas\n",
    "  data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "  X = data\n",
    "\n",
    "  y = data.pop('species').values\n",
    "\n",
    "  # Divisão dos dados em conjuntos de treino e teste na proporção de 80% para treino e 20% para teste\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prepareDataMultipleClassification():\n",
    "  data = pd.read_csv('prepareData/mobile_price_multiple_classification.csv')\n",
    "\n",
    "  # print(data.head())\n",
    "  \n",
    "  numerical_cols = ['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g', 'touch_screen',  'wifi']\n",
    "\n",
    "  # Inicializando o MinMaxScaler\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "  # Aplicando a normalização às colunas numéricas\n",
    "  data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "  \n",
    "  X = data.drop(columns=['price_range'])\n",
    "\n",
    "  y = pd.get_dummies(data[\"price_range\"], prefix=\"price\", sparse=False)\n",
    "\n",
    "  # Divisão dos dados em conjuntos de treino e teste na proporção de 80% para treino e 20% para teste\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "  \n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "def prepareDataRegression():\n",
    "  DATA_PATH = 'prepareData/house_price_regression_dataset.csv'\n",
    "\n",
    "  data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "  # print(data.head())\n",
    "\n",
    "  numerical_cols = ['Square_Footage', 'Num_Bedrooms', 'Num_Bathrooms', 'Year_Built', 'Lot_Size', 'Garage_Size', 'Neighborhood_Quality', 'House_Price']\n",
    "\n",
    "  min_max_house_price = (data['House_Price'].min(), data['House_Price'].max())\n",
    "\n",
    "  # Inicializando o MinMaxScaler\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "  # Aplicando a normalização às colunas numéricas\n",
    "  data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "  X = data\n",
    "\n",
    "  y = data.pop('House_Price').values\n",
    "\n",
    "  \n",
    "  # Divisão dos dados em conjuntos de treino e teste na proporção de 80% para treino e 20% para teste\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "  return X_train, X_test, y_train, y_test, min_max_house_price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b8b0f",
   "metadata": {},
   "source": [
    "# Montagem da Rede de Classificação Multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd39c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 20\n",
    "\n",
    "layers = [\n",
    "    {'neurons': 10, 'activation_function': 'relu'},\n",
    "    {'neurons': 10, 'activation_function': 'relu'},\n",
    "    {'neurons': 4, 'activation_function': 'softmax'}\n",
    "]\n",
    "\n",
    "ann_layer = initialize_layers(layers, c_inputs=input_size)\n",
    "\n",
    "x_train, x_test, y_train, y_test = prepareDataMultipleClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "023306ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = train(\n",
    "    ann=ann_layer,\n",
    "    epochs=10,\n",
    "    x=x_train,\n",
    "    y=np.array(y_train),\n",
    "    learning_rate=0.002,\n",
    "    cost_func_name='categorical_cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bc2363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Resultados da Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88       114\n",
      "           1       0.97      0.34      0.50       103\n",
      "           2       0.50      0.49      0.49        80\n",
      "           3       0.71      0.99      0.83       103\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       400\n",
      "   macro avg       0.74      0.70      0.68       400\n",
      "weighted avg       0.76      0.72      0.69       400\n",
      " samples avg       0.72      0.72      0.72       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(ann, x, y):\n",
    "    predictions = []\n",
    "    for observation_id in range(len(x)):\n",
    "        input = np.array(x.iloc[observation_id])\n",
    "        prediction, _ = feed_forward(ann, input)\n",
    "        one_hot_prediction = np.zeros_like(prediction)\n",
    "        one_hot_prediction[np.argmax(prediction)] = 1\n",
    "        predictions.append(one_hot_prediction)\n",
    "    \n",
    "    print(\"\\n Resultados da Classificação:\")\n",
    "    print(classification_report(y, predictions))\n",
    "    \n",
    "evaluate(ann, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491b009",
   "metadata": {},
   "source": [
    "# Montagem da Rede de Classificação Binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5f90e",
   "metadata": {},
   "source": [
    "# Montagem da Rede de Regressão"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
